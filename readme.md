# NN Descent

因工作后续会涉及, 用python 实现NN Descent 来进行跨层聚类和更灵活的自定义距离.

从目前结果来看, 和sklearn 的knn 相比, 该方法还是有一定优势:
- 当N 比较大(例如5w)时, 自定义2范数的速度比Knn 默认2范数快10s. 
- knn 中自定义距离和weight 无法引入层级标签信息, 但nn Descent 可以.

缺点:
- 感觉比较依赖初始值设定.
- 从结果来看, 精度还是不如knn. 不过在通过knn graph 获得预期标签的场景下, 二者的差距在平均意义上应该不大.
- 目前实现的代码还有可以优化的部分, 比如过度依赖sortList 、 没有多线程并行(从之前尝试来看, 效果好像不大)、MapReduce版本实现、Cython 优化距离函数等等. 如果后续使用到这个算法再进行优化和修正吧.